---
title: 'Road to RKHS'
date: 2020-05-22
permalink: /posts/2020/05/blog-post-4/
tags:
  - RKHS
  - functional analysis
  - machine learning
---

Reproducing Kernel Hilbert Space(RKHS) is related to kernel trick in machine learning and plays a significant role in representation learning. In this post, I will summarize the mathematical definition of RKHS.

First, we define the notion of a metric space. a metric space is defined as following:

Let $S$ be a set, and suppose $d$ is a function defined for all pairs $(x, y)$ of elements from $S$ satisfying:
1. Nonnegativity: $d(x, x) = 0 \forall x \in S$ and $d(x, y) > 0$ for distinct $x, y \in S$
2. Symmetry: $d(x, y) = d(y, x) \forall x, y \in S$
3. Triangle Inequality: $d(x, z) \leq d(x, y) + d(y, z) \forall x, y, z \in S$

Such a function $d$ is called a distance function or a metric on $S$. A metric space $S$ is a set $S$ together with a distance metric $d$ on it. A metric space is often written as a pair $(S, d)$. To summarize, a metric space is a set and the distance between it's elements can be measured by the distance metric $d$. This means $S$ has a topological structure, which enables us to study notions of continuity and convergence.

Now we introduce the notion of a linear space. A set $S$ or elements $x, y, z, ...$ is a linear space if the following conditions are satisfied:

