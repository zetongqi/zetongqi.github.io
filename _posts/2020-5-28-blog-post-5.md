---
title: 'Learning with Invariances via RKHS'
date: 2020-05-28
permalink: /posts/2020/05/blog-post-5/
tags:
  - RKHS
  - functional analysis
  - machine learning
---

In the [previous post](https://zetongqi.github.io/posts/2020/05/blog-post-4/) I introduced the concept and properities of Reproducing Kernel Hilbert Space(RKHS). In this post, I will introduce how to construct RKHS from a kernel function and build learning machines that can learn invariances encoded in the form of bounded linear functionals on the constructed RKHS.

Given a learning problem, suppose the features of training examples lie in a domain $\mathcal{X}$. A function $k: \mathcal{X} \times \mathcal{X} \to \mathcal{R}$ is called a positive semi-definite kernel if the $\forall l \in \mathcal{N}$ and $\forall x_1, x_2, \dots, x_l$ the $l \times l$ gram matrix it produces: $K[i, j] \vcentcolon= k(x_i, x_j)$ is symmetric PSD.

Therefore, given a kernel function $k$, we can constructing a RKHS this way: let $\mathcal{H}$ be a set of all finite linear combinations of functions in $\{ k{x, \cdot}, x \in \mathcal{X} \}$, that is, $\{ k{x, \cdot}, x \in \mathcal{X} \}$ span the entire $\mathcal{H}$. Define an inner product on $\mathcal{H}$ as $\langle f, g \rangle = \sum_{i=1}^p \sum_{j=1}^q \alpha_i \beta_j k(x_i, y_j)$ where $f(\cdot) = \sum_{i=1}^p \alpha_i k(x_i, \cdot)$ and $g(\cdot) = \sum_{j=1}^p \beta_j k(y_j, \cdot)$. Let $\mathcal{H}$ also be complete under the inner product $\langle \cdot, \cdot \rangle$, then $\mathcal{H}$ is an RKHS induced by kernel function $k$. For any $f \in \mathcal{H}$, the reproducing properity states that $f(x) = \langle f, K(x, \cdot) \rangle$.

Recall that a functional maps from a vector space to $\mathcal{R}$. We now show that a functional on RKHS is bounded. Let $\mathcal{H}$ be a RKHS induced by a kernel $k$ defined on $\mathcal{X}^2$. Then $\forall x \in \mathcal{X}$, the linear functional $T: \mathcal{H} \to \mathcal{R}$ defines as $T(f) = \langle f, k(x, \cdot) \rangle = f(x)$ is bounded since $\mid \langle f, k(x, \cdot) \rangle \mid \leq \lVert k(x, \cdot) \rVert \cdot \lVert f \rVert = k(x, x)^{\frac{1}{2}} \lVert f \rVert$ by Cauchy-Schwarz inequality.

Let's now look at Riesz representation theorem. Riesz representation theorem says that for a linear functional $L$ on a Hilbert space $\mathcal{H}$, it's evaluation of a member of $\mathcal{H}$ can be represented as a inner product: $L(f) = \langle f, z \rangle$ where $z$ is the representer of the funcional and $z \in \mathcal{H}$. $z$ also has norm: $\lVert z \rVert = \lVert L \rVert$ and is uniquely determined by $L$.

For example, let $\mathcal{H}$ be an RKHS induced by a kernel $k$, for any functional $L$ on $\mathcal{H}$, the representer $z$ can be constructed as $z(x) = \langle z, k(x, \cdot) \rangle$ $\forall x \in \mathcal{X}$.

Now, let's define a semi-supervised learning problem and see how the framework applies to this learning problem. In semi-supervised learning, we have some labeled data and a lot of unlabeled data and we wish to learn a classifier with high accuracy, robustness and good generalization. This problem comes up a lot in learnings where labeled data are expensive to obtain. To define a loss function for optimization, let $(x_1, y_1), \dots, (x_l, y_l)$ be the set of labeled data, and let $\mathcal{l_1}(f(x), y)$ be a convex loss function (e.g. logistic loss, hinge loss and squared loss). This term will be the penality induced by the labeling function $f$ when $f$ mislabels a $x$.

We also measure how much our target labeling function $f$ disatisfies the invariances constraints, and denote them as $L_{l+1}(f), \dots, L_{l+m}(f)$. For semi-supervised learning problems and a lot of other learning problems as well, to learn a good classifier, we will want to encode our prior belief into the learning problem. One reasonable such prior belief is that the gradients of the labeling function in all directions at each labeled and unlabeld instances are small. That is, $f$ doesn't change very rapidly around each instance. This is a reasonable since instances belonging to the same class tend to cluster together. This will encourage the optimizer to search for an $f$ whose decision boundary is in a low data density reagion since the $f$ tends to change rapidly around decision boundary($f$ has large gradients near decision boundary). We associate another convex loss function with those linear functionals $\mathcal{l_2}(L_i(f))$, this can be squared loss, absolute loss and $\epsilon$-sensitive loss.

Finally, according to Occam's razor which can be described as the following philosophical message: A short explanation (that is, a hyphothesis that has a short length) tends to be more valid than a long explanation. We also penalize the complexity of $f$ via the RKHS norm ${\lVert f \rVert}^2$. Now we have the following optimization problem:

$$\min_{f \in \mathcal{H}} \frac{1}{2} {\lVert f \rVert}^2 + \lambda \sum_{i=1}^l \mathcal{l_1}(f(x), y) + \nu \sum_{i=l+1}^{l+m} \mathcal{l_2}(L_i(f))$$

Where $\lambda, \nu > 0$. Due to the convexity of $\mathcal{l_1}$ and $\mathcal{l_2}$, the above optimization is convex. We now make the following claim that establishes the form of the optimal solution to the above optimization problem:

Let $\mathcal{H}$ be the RKHS induced by kernel $k$. Let $L_i (i=l+1, \dots, l+m)$ be bounded linear functionals on $\mathcal{H}$ with representers $z_i$, the optimal solution to the above optimization must be in the form of:

$$g(\cdot) = \sum_{i=1}^l \alpha_i k(x_i, \cdot) + \sum_{i=l+1}^{l+m} \alpha_i z{\cdot}$$

Therefore, the parameters $\alpha = (\alpha_1, \dots, \alpha_{l+m})'$ (finite dimensional) can be found by minimizing:

$$\lambda \sum_{i=1}^l \mathcal{l_1} (\langle k(x_i, \cdot), f \rangle, y_i) + \nu \sum_{i=l+1}^{l+m} \mathcal{l_2} (\langle z_i, f \rangle) + \frac{1}{2} \alpha' K \alpha$$

where $f = \sum_{i=1}^l \alpha_i k(x_i, \cdot) + \sum_{i=l+1}^{l+m} \alpha_i z{\cdot}$ and K()

















