---
title: 'Learning with Invariances via RKHS'
date: 2020-05-28
permalink: /posts/2020/05/blog-post-5/
tags:
  - RKHS
  - functional analysis
  - machine learning
---

In the [previous post](https://zetongqi.github.io/posts/2020/05/blog-post-4/) I introduced the concept and properities of Reproducing Kernel Hilbert Space(RKHS). In this post, I will introduce how to construct RKHS from a kernel function and build learning machines that can learn invariances encoded in the form of bounded linear functionals on the constructed RKHS.

Given a learning problem, suppose the features of training examples lie in a domain $\mathcal{X}$. A function $k: \mathcal{X} \times \mathcal{X} \to \mathcal{R}$ is called a positive semi-definite kernel if the $\forall l in \mathcal{N}$ and $\forall x_1, x_2, \dots, x_l$ the $l \times l$ gram matrix it produces: $K[i, j] \vcentcolon= k(x_i, x_j)$ is symmetric PSD.

Therefore, given a kernel function $k$, we can constructing a RKHS this way: let $\mathcal{H}$ be a set of all finite linear combinations of functions in $\{ k{x, \cdot}, x \in \mathcal{X} \}$, that is, $\{ k{x, \cdot}, x \in \mathcal{X} \}$ span the entire $\mathcal{H}$. Define an inner product on $\mathcal{H}$ as $\langle f, g \rangle = \sum_{i=1}^p \sum_{j=1}^q \alpha_i \beta_j k(x_i, y_j)$ where $f(\cdot) = \sum_{i=1}^p \alpha_i k(x_i, \cdot)$ and $g(\cdot) = \sum_{j=1}^p \beta_j k(y_j, \cdot)$. Let $\mathcal{H}$ also be complete under the inner product $\langle \cdot, \cdot \rangle$, then $\mathcal{H}$ is an RKHS induced by kernel function $k$. For any $f \in \mathcal{H}$, the reproducing properity states that $f(x) = \langle f, K(x, \cdot) \rangle$

Recall that a functional maps from a vector space to $\mathcal{R}$. We now show that a functional on RKHS is bounded. Let $\mathcal{H}$ be a RKHS induced by a kernel $k$ defined on $\mathcal{X}^2$. Then $\forall x \in \mathcal{X}$, the linear functional $T: \mathcal{H} \to \mathcal{R}$ defines as $T(f) = \langle f, k(x, \cdot) \rangle = f(x)$ is bounded since $|\langle f, k(x, \cdot)| \leq \lVert k(x, \cdot) \rVert \cdot \lVert f \rVert = k(x, x)^{\frac{1}{2}} \lVert f \rVert$ by Cauchy-Schwarz inequality.

Let's now look at Riesz representation theorem. Riesz representation theorem says that for a linear functional $L$ on a Hilbert space $\mathcal{H}$, it's evaluation of a member of $\mathcal{H}$ can be represented as a inner product: $L(f) = \langle f, z \rangle$ where $z$ is the representer of the funcional and $z \in \mathcal{H}$. $z$ also has norm: $\lVert z \rVert = \lVert L \rVert$ and is uniquely determined by $L$.

For example, let $\mathcal{H}$ be an RKHS induced by a kernel $k$, for any functional $L$ on $\mathcal{H}$, the representer $z$ can be constructed as $z(x) = \langle z, k(x, \cdot) \rangle$ $\forall x \in \mathcal{X}$.

Now, let's define a semi-supervised learning problem and see how the framework applies to this learning problem. In semi-supervised learning, we have some labeled data and a lot of unlabeled data and we wish to learn a classifier with high accuracy, robustness and good generalization. This problem comes up a lot in learnings where labeled data are expensive to obtain. To define a loss function for optimization, let $(x_1, y_1), \dots, (x_l, y_l)$ be the set of labeled data, and let $\mathcal{l_1}(f(x), y)$

















