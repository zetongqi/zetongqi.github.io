---
title: 'Finite-dimensional Approximation of Kernel SVM with Random Fourier Features'
date: 2020-06-16
permalink: /posts/2020/06/blog-post-7/
tags:
  - support vector machines
  - kernel trick
  - machine learning
---

Support vector machines in my opinion the best machine learning algorithm. It generalizes well with less risk to overfitting, it scales well to high-dimensional data, and kernel trick makes it possible to efficiently lift the feature space to higher dimensions, and the optimization problem for SVMs are usually quadratic programs that are efficient to solve and has a global minimum. However, there are a few drawbacks to SVMs. The biggest one is that it doesn't scale well with large dataset and the inference time depends on the size of the training dataset. For a training set of set $m$, where $x \in \mathcal{R}^d$, the time complexity to compute the gram matrix is $\mathcal{O}(m^2 d)$ and the time complexity to compute $\mathcal{O}(md)$. This time complexity scales poorly with large training sets.

To address this issue, we can approximate the kernel functions using finite-dimensional appromixation. That is, we can explicitly map the data to a low-dimensional Euclidean inner product space using a randomized feature map $z: \mathcal{R}^d \to \mathcal{R}^D$ so that the inner product between pairs of transformed points approximates their kernel evaluation: $k(x, y) = \langle \phi(x) \phi(y) \rangle \approx z(x)'z(y)$.

Unlike the lifting $\phi(\cdot)$, $z(\cdot)$ is low-dimensional, therefore we can just simply transform the input with $z(\cdot)$, and then use fast linear methods to approximate the answer of the corresponding nonlinear kernel machine.

The finite-dimensional approximation only works for approximating shift invariant kernels, that is, kernel functions whose value only depends on the difference between two points $x$ and $y$. Popular kernel functions such as Gaussian kernels and Laplacian kernels are all shift invariant kernels.

How to approximate a nonlinear kernel function with potentially infinite dimensions using finite-dimensional mapping? The following classic theorem from harmonic analysis is the foundation for this transformation:

Bochner. A continuous kernel $k(x, y)=k(x-y)$ on $\mathcal{R}^d$ is positive definite if and only if $k(\delta)$ is the Fourier transform of a non-negative measure.

That is, if a shift-invariant kernel $k(\delta)$ is properly scaled, Bochner's theorem guarantees that its Fourier transform $p(\omega)$ is a proper probability distribution. Let $\zeta_{\omega}(x) = e^{j\omega' x}$:

$$k(x-y) = \int_{\mathcal{R}^d} p(\omega) e^{j\omega' (x-y)} = \mathcal{E}[ \zeta_{\omega}(x)\zeta_{\omega}(y)^{\*}]$$

And $\zeta_{\omega}(x)\zeta_{\omega}(y)^{\*}$ is an unbiased estimate of $k(x, y)$ when $\omega$ is drawn from $p$. According to [Ali Rahimi el.al.](https://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf), (1) converges when the complex exponentials are replaced with cosines. Therefore, we obtain a real-valued mapping that satisfies the condition $\mathcal{E}[ \zeta_{\omega}(x)\zeta_{\omega}(y)] = k(x, y)$ by setting $z_{\omega}(x) = \sqrt{2} cos(\omega'x+b)$ where $\omega$ is drawn from $p(\omega)$ and $b$ is drawn uniformaly from $[0, 2\pi]$











