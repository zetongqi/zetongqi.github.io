---
title: 'Variational Autoencoders'
date: 2020-07-18
permalink: /posts/2020/07/blog-post-8/
tags:
  - deep learning
  - representation learning
  - machine learning
---

Autoencoder is a type of neural network that learns a lower dimensional latent representation of input data in an unsupervised manner. The learning task is simple: given an input image, the network will try to reconstruct the image at the output. The loss is measured by the distance between two images, e.g., MSE loss.

Variational autoencoder improves upon this idea, instead of mapping the inputs to a latent representation using a fixed transformation, variational autoencoder tries to map the inputs to a distribution of latent features. What does that mean? Consider the picture below that illustrates how autoencoder maps the inputs to a latent space:

<br/><img src='/images/blog_post_images/autoencoder.png' width="400">

Instead of mapping the inputs to a fixed vector of latent features, VAE maps it to a distribution of latent features:

<br/><img src='/images/blog_post_images/vae.png' width="400">

How does VAE map inputs to distributions? VAE assumes each latent features is Gaussian distributed, therefore is network is learning MLP layes that maps the input to a mean $\mu \in \mathcal{R}^d$ and variance $\sigma \in \mathcal{R}^{d \times d}$ where $d$ is the dimensionality of the latent space. VAE then samples a latent representation from this Multivariate Gaussian distribution and feed it to the decoder, which is a determinstic mapping, to reconstruct the input image. The architecture of VAE is shown below:

<br/><img src='/images/blog_post_images/vae_architecture.png' width="400">

$\mu$ and $\sigma$ are parameters in the network that can be learned with gradient methods. In order to understand more about VAE, lets now turn to it's statistical motivation. suppose we have a graphcal model:

<br/><img src='/images/blog_post_images/variational_inference_graphcal_model.png' width="50">

Where there exists some latent variable $z$ that generates $x$. We can only observe $x$ but we would like to know more about $z$. In other words, we would like to compute $p(z\|x)$:

$$p(z|x) = \frac{p(x|z)p(z)}{p(x)}$$

However, $p(x) = \int p(x\|z) p(z) dz$ is hard to compute and intractable. We can use variational inference and choose a distribution of simpler form (e.g. a family of Gaussian distribution) to approximate this intractable distribution. Let's define the relationship between the inputs $x$ and the latent encoding vector $z$ by:

prior: $p_{\theta}(z)$

Posterior: $p_{\theta}(z\|x)$

Where the distribution is parameterized by $\theta$. Let's use $q_{\phi}(z\|x)$ to approximate the intractable $p_{\theta}(z\|x)$. $q_{\phi}(z\|x)$ is parameterized by $\phi$. Since we want to use $q_{\phi}(z\|x)$ to approximate $p_{\theta}(z\|x)$, we want to minimize the KL divergence between $q_{\phi}(z\|x)$ and $p_{\theta}(z\|x)$:

$$\underset{\phi}{\text{min}} KL(q_{\phi}(z|x) || p_{\theta}(z|x))$$

Expanding the KL divergence term, we have:

$KL(q_{\phi}(z\|x) \|\| p_{\theta}(z\|x)) = \int q_{\phi}(z\|x) \log \frac{q_{\phi}(z\|x)}{p_{\theta}(z\|x)} dz$

$= \int q_{\phi}(z\|x) \log \frac{q_{\phi}(z\|x) p_{\theta}(x)}{p_{\theta}(z, x)} dz = \int q_{\phi}(z\|x) (\log p_{\theta}(x) + \log \frac{q_{\phi}(z\|x)}{p_{\theta}(z, x)})dz$

$= \log p_{\theta}(x) + \int q_{\phi}(z\|x) \log \frac{q_{\phi}(z\|x)}{p_{\theta}(z, x)}dz$

Rearranging the terms above, we have:

$$\log p_{\theta}(x) = KL(q_{\phi}(z|x) || p_{\theta}(z|x)) - \int q_{\phi}(z\|x) \log \frac{q_{\phi}(z\|x)}{p_{\theta}(z, x)}dz$$

Or:

$$\log p_{\theta}(x) = KL(q_{\phi}(z|x) || p_{\theta}(z|x)) + \int q_{\phi}(z|x) \log \frac{p_{\theta}(z, x)}{q_{\phi}(z|x)}dz$$

Where the LHS is a constant (the log likelihood of the data is constant), on the RHS, the KL divergence term is what we are trying to minimize. Instead of minimizing KL divergence term, we can maximizing the second term $\int q_{\phi}(z\|x) \log \frac{p_{\theta}(z, x)}{q_{\phi}(z\|x)}dz$, which is called the variational lower bound, which we will denote as $\mathcal{L}$. The reason $\mathcal{L}$ is called the variational lower bound is because the KL divergence term is non-neggative, therefore $\mathcal{L}$ is a lower bound of the log likelihood $log p_{\theta}(x)$. A different way to look at this optimization problem is that since $\mathcal{L}$ is a lower bound of the log likelihood, maximizing the lower bound also maximizes the log likelihood itself.

If we further expand the variational lower bound $\mathcal{L}$, we have:

$\mathcal{L} = \int q_{\phi}(z\|x) \log \frac{p_{\theta}(z, x)}{q_{\phi}(z\|x)}dz = \int q_{\phi}(z\|x) \log \frac{p_{\theta}(x\|z)p_{\theta}(z)}{q_{\phi}(z\|x)}dz$

$= \int q_{\phi}(z\|x) \log p_{\theta}(x\|z)dz - \int q_{\phi}(z\|x) \log \frac{q_{\phi}(z\|x)}{p_{\theta}(z)}dz$

$= E_{z \sim q_{\phi}(z\|x)} \[\log p_{\theta}(x\|z)\] - KL(q_{\theta}(z\|x) \|\| p_{\theta}(z))$

Since the decoder network is determinstic, we can write the first term as:

$E_{z \sim q_{\phi}(z\|x)} \[\log p_{\theta}(x\|z)\] = E_{z \sim q_{\phi}(z\|x)} \[\log p_{\theta}(x\|\hat{x})\]$

If we assume $p_{\theta}(x\|\hat{x})$ has the from of: $p_{\theta}(x\|\hat{x}) = e^{-(x-\hat{x})^2}$, the log likelihood would become maximizing the negative MSE loss between the input and the reconstruction:

$\mathcal{L} = -(x-\hat{x})^2 - KL(q_{\theta}(z\|x) \|\| p_{\theta}(z))$











