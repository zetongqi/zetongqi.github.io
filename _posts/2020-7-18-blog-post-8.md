---
title: 'Variational Autoencoders'
date: 2020-07-18
permalink: /posts/2020/07/blog-post-8/
tags:
  - deep learning
  - representation learning
  - machine learning
---

Autoencoder is a type of neural network that learns a lower dimensional latent representation of input data in an unsupervised manner. The learning task is simple: given an input image, the network will try to reconstruct the image at the output. The loss is measured by the distance between two images, e.g., MSE loss.

Variational autoencoder improves upon this idea, instead of mapping the inputs to a latent representation using a fixed transformation, variational autoencoder tries to map the inputs to a distribution of latent features. What does that mean? Consider the picture below that illustrates how autoencoder maps the inputs to a latent space:

<br/><img src='/images/blog_post_images/autoencoder.png' width="400">

Instead of mapping the inputs to a fixed vector of latent features, VAE maps it to a distribution of latent features:

<br/><img src='/images/blog_post_images/vae.png' width="400">

How does VAE map inputs to distributions? VAE assumes each latent features is Gaussian distributed, therefore is network is learning MLP layes that maps the input to a mean $\mu \in \mathcal{R}^d$ and variance $\sigma \in \mathcal{R}^{d \times d}$ where $d$ is the dimensionality of the latent space. VAE then samples a latent representation from this Multivariate Gaussian distribution and feed it to the decoder, which is a determinstic mapping, to reconstruct the input image. The architecture of VAE is shown below:

<br/><img src='/images/blog_post_images/vae_architecture.png' width="400">














