---
title: 'Variational Autoencoders'
date: 2020-07-18
permalink: /posts/2020/07/blog-post-8/
tags:
  - deep learning
  - representation learning
  - machine learning
---

Autoencoder is a type of neural network that learns a lower dimensional latent representation of input data in an unsupervised manner. The learning task is simple: given an input image, the network will try to reconstruct the image at the output. The loss is measured by the distance between two images, e.g., MSE loss.

Variational autoencoder improves upon this idea, instead of mapping the inputs to a latent representation using a fixed transformation, variational autoencoder tries to map the inputs to a distribution of latent features. What does that mean? Consider the picture below that illustrates how autoencoder maps the inputs to a latent space:

<br/><img src='/images/blog_post_images/autoencoder.png' width="400">

Instead of mapping the inputs to a fixed vector of latent features, VAE maps it to a distribution of latent features:

<br/><img src='/images/blog_post_images/vae.png' width="400">

How does VAE map inputs to distributions? VAE assumes each latent features is Gaussian distributed, therefore is network is learning MLP layes that maps the input to a mean $\mu \in \mathcal{R}^d$ and variance $\sigma \in \mathcal{R}^{d \times d}$ where $d$ is the dimensionality of the latent space. VAE then samples a latent representation from this Multivariate Gaussian distribution and feed it to the decoder, which is a determinstic mapping, to reconstruct the input image. The architecture of VAE is shown below:

<br/><img src='/images/blog_post_images/vae_architecture.png' width="400">

$\mu$ and $\sigma$ are parameters in the network that can be learned with gradient methods. In order to understand more about VAE, lets now turn to it's statistical motivation. suppose we have a graphcal model:

<br/><img src='/images/blog_post_images/variational_inference_graphcal_model.png' width="50">

Where there exists some latent variable $z$ that generates $x$. We can only observe $x$ but we would like to know more about $z$. In other words, we would like to compute $p(z\|x)$:

$$p(z\|x) = \frac{p(x\|z)p(z)}{p(x)}$$

However, $p(x) = \int p(x\|z) p(z) dz$ is hard to compute and intractable. We can use variational inference and choose a distribution of simpler form (e.g. a family of Gaussian distribution) to approximate this intractable distribution. Let's define the relationship between the inputs $x$ and the latent encoding vector $z$ by:

prior: $p_{\theta}(z)$
Posterior: $p_{\theta}(z\|x)$

Where the distribution is parameterized by $\theta$. Let's use $q_{\phi}(z\|x)$ to approximate the intractable $p_{\theta}(z\|x)$. $q_{\phi}(z\|x)$ is parameterized by $\phi$. Since we want to use $q_{\phi}(z\|x)$ to approximate p_{\theta}(z\|x)$, we want to minimize the KL divergence between $q_{\phi}(z\|x)$ and p_{\theta}(z\|x)$:

$$\phi^* = argmin_{\phi} KL(q_{\phi}(z\|x) || p_{\theta}(z\|x))$$












